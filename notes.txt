OK, database location is specified as a directory path, where the library
controls the files for whatever needs it has.

---
Do we need any metadata assigned to edges?

We need to express that this revision is a child of another revision, because
it's a new revision of a patchset. OTOH, the fact that a revision's edge
target is a patchset says that this revision is a new version of one. No
matter if this revision is a git commit or a patchset itself.

So, let's say we have a series of patchset versions each based on a particular
commit, which then gets merged as the following commit, with a few commits on
top:

[a]
 ^
[b]
 ^
[c] <- (1)
 ^      ^
 |-----(2)
 |      ^
 |-----(3)
 |      ^
 |-----(4)
 |      ^
 `-----[d]
        ^
       [e]
        ^
       [f]

What if the patchset version (3) fixes an issue which was introduced in
commit [b]?

In the absence of other information we're supposed to say that the branch has
the issue fixed now.

However, if we travel down both edges from [d], one will contain the issue,
and another would not. At the same time, the fix would be the same distance
from it as the issue itself. What should we say? Is the issue there, or not?

We could say that since there's a commit without the issue *after* the one
with it, it's *probably* fixed. So the test would be get all closest mentions
of issues, and if there are any ambiguities, check which one is before which.

If we can't "see" the issue from where we are (i.e. a revision with it
reproducing is masked by another with it *not* reproducing), then we can say
the issue is *likely* fixed.

We can only say that the issue is "definitely" fixed if we tested the
particular commit.

Here's a more involved history:


[a]
 ^
[b] <- (1)
 ^      ^
[c]     |
 ^      |
[d] <- (2)
 ^      ^
[e]     |
 ^      |
[f] <- (3)
 ^      ^
[g]     |
 ^      |
[h] ----'
 ^
[i]

So, for [i], if an issue is detected in [e], and not in (2), then it's still
there.

Or, if an issue is detected in [e], but not (3), then it's probably not there.

Then, if an issue is detected in (3), but not in [h], then it's likely not
there.

Finally if an issue is detected in [a], but not in [i] itself, then it's
definitely not there.

In all these cases, we don't actually need to know if a revision is a commit
or a patchset.

What about merge requests getting merged?

Can we record them like this:

[a]
 ^
[b] <- [A1] < [B1] < [C1]
 ^                    ^
[c]                   |
 ^                    |
[d] <- [A2] < [B2] < [C2]
 ^                    ^
[e]                   |
 ^                    |
[f]                   |
 ^                    |
[g] <- [A3] < [B3] < [C3]
 ^                    ^
[h]                   |
 ^                    |
[i] ------------------'
 ^
[j]

Well, first of all the merge requests consist of actual commits themselves,
and then they can be actually merged, or simply rebased and fast-forwarded to.

In this case, if [B2] was found to fix an issue, then we can assume that [C3]
has the fix (but note, not B3, with this model), and therefore [i] can be
assumed to contain the fix as well.

Here we have both actual merge and a succession of the merge request version
expressed with the same type of edges, and we have no way of distinguishing
them.

Should we introduce edge types, and if yes, how are we going to use them in
queries? Bitmasks? Numbers?

Should we have more edge types? E.g. between releases? The latter, probably
not, as they're connected after all.

Nooooo... This would require us to build separate generation numbers, and
bloom filters for different sets of edges. So perhaps we can mark the
revisions somehow instead? E.g. say if both the parent and the child are merge
request heads, then the edge is an MR version update? Hmm, that's actually not
necessarily true. What if an MR version was fast-forwarded to, then the next
MR was a single commit, which got fast-forwarded to as well? What would the
edge between them signify? Ah, they would have to have the same MR IDs
assigned to version heads to qualify as versions of the same MR.

So, no we won't have different types of edges. That would be too much.

I wonder if we could simply have separate graphs for separate edge types?
Hmm, that could do it!
---
OK, how do we fill a newly-created HDAG file with data?

We can supply it at the moment of creation.
Afterwards we don't allow changing it, except deleting nodes (marking them
unknown).

Perhaps the hdag_file_create can accept a callback which would return the node
data, plus a function for retrieving its outgoing edges.

We shouldn't expect to know the number of nodes from the start, because we
need a node allocated for targets as well, which might not be in the nodes (be
unknown at the moment, or be in another HDAG file?).

Aaah, and here we need to know which nodes are already present in other files
and which not. That means that the database needs to look them up through all
the HDAG files. That also means that the database can take care of sorting and
arranging the data for each file. And the hdag_file_create can accept
pre-cooked data, and can be kept simple. Or even the database code can fill in
whatever is needed itself, once the file is created. This would mean that the
file module wouldn't need to think about locking, consistency, or anything
else. Can we do that? Well, it would be good if it was the job of the file
module to take care of internal consistency, and for database module to take
care overall database consistency (no duplicates, no loops, and so on).

Although, the file module would also need to take care of loops. Does it make
easier the database's job of looking out for loops? Say, it could look for
unknown nodes in each file and see if they're present in other files. If
a node in one file refers to a node in another file, then they should have the
same component ID. That should be arranged by the database code.

We need to distinguish the components, so that we can compare generation
numbers.

Yes, the file module would need to do sorting as well, or at least verify it,
because it has to provide the interface for looking up nodes.

We don't need an online sorting algorithm when ingesting nodes, as our storage
structure cannot be reshuffled efficiently, and so we cannot use it as our
storage for sorting in progress. We would need to accumulate the received
nodes, then sort them, and then output them into our storage sorted.
---
OK, we don't know how many node entries we'll have in the file, so we need to
either store the incoming data in a separate structure, or be able to resize a
file. The problem with sorting the file structure is updating the target
references. In particular, we have to walk them back, which the file has no
optimization for.

So, perhaps we would need to sort them in a separate structure, after all.
And then we don't really need to make the file resizeable.

We can allow supplying both an adjacency list and an edge list when creating
the file, because we would have to have an edge list anyway to avoid
variable-length node records, or updating references.

Hmm, actually, perhaps each node in this representation can reference the
starting and the ending indices in a separate list of targets instead?

This way we can first fill up the unsorted node list... no...

We would have a problem with consuming a separate edge list, because we won't
know how many targets a node would have, only after we've seen the whole list,
and that would require us to reshuffle the list constantly as we're consuming
it.

So let's drop it and just consume the adjacency list.

So:
    * Fill in a (deduplicated) array of node hashes:
        * The hashes returned by node_seq
        * The hashes returned by hash_seq

    NOTE: We gotta mark hashes for which we haven't received a node yet,
          somehow

    * Fill in an array of target hashes

    * Sort the node hash array.

Actually, we *can* use struct hdag_file_node, and simply always use target
list references. But we would need a different target list from the one in the
file, as it should contain hashes, rather than indexes into the node list.

OK, which sorting approach do we take? If we use qsort, the performance should
be good, since the data is randomly distributed, but we will need to store the
list of all nodes and *all* targets, because deduplicating requires lookup.

I.e. we just store all node hashes, and all target hashes, without
deduplicating, then we sort, then we deduplicate. In general, in git history
graphs, there's not much duplication of target nodes. So it probably won't be
a big deal. Ah, wait, we would need at least 2N-1 memory to put both the
nodes and their targets in the same array, without deduplication. So, maybe we
better use skip list sort for online sorting and deduplication.

OK, another try:

    For each received node and its adjacency list
        Insert

Ah, wait, if we're going to simply put *all* the target hashes into an array,
without packing most of them into the node, then we would take a similar
amount of memory as the qsort approach does.

OK, let's start with the simplest, qsort approach.

So:

    For each received node and its adjacency list
        Remember the length of the target array
        For each hash in the adjacency list
            Add an entry into the node array:
                hash
                first target index = unknown
                second target index = unknown
            Add an entry into the target array:
                hash

        Add node info to the array:
            hash
            if current target array length == remembered length
                first target index = invalid
                second target index = invalid
            else
                first target index = remembered length
                second target index = current length - 1

    Sort node array

---

Sooo, how would we deduplicate the preprocessed array of nodes?

We need to leave only one node of each hash. After sorting they will be put
together.

If there's a node with known targets among same-hash nodes, it should be used.
Otherwise, any node with unknown targets should used.
---
Add nodes and target hashes to corresponding arrays, as they come in
Sort nodes by their hashes
De-duplicate nodes by their hashes
Compact the targets
---
# Compacting
for each node, start to end
    if node targets are unknown
        continue
    if there's more than two of known targets
        set node's first target to indirect index of next available extra edge slot
        for each target hash
            lookup node index
            add an extra edge with that target index
        set node's second target to indirect index of the last taken extra edge slot
    else
        for each target hash
            lookup node index
            store it in the node's targets as direct index

---
TODO: Rename 'edges' to 'extra_edges' in the file.
TODO: Use 'nodes_num' and 'extra_edges_num' for consistency.
TODO: Consider renaming HDAG_TARGET_INVALID to HDAG_TARGET_NONE, or
      HDAG_TARGET_ABSENT.
        - done!
TODO: Deduplicate edges.
        - done!
TODO: See if we could normalize on the "destination hash", instead of "target
      hash", or vice versa.
TODO: Consider replacing "ind_extra_edges" with looking at which of
      target_hashes/extra_edges is non-empty.
        - done!
---
ASSIGNING GENERATION AND COMPONENT NUMBERS

Create an array of node indices able to accommodate all nodes (maybe minus
one?). Use it as a stack to remember the nodes to go back to during the DFS.
Actually, just resize it as necessary, in case we have a "wide" graph.

Store the number of already traversed outgoing edges in the node's
"generation".

Store the node's generation in "generation", once all edges are traversed and
we are returning to the source node.

Use two distinct ranges of "generation" to store traversal state and the
generation itself. Both excluding zero.

Thus a value in the second range (the actual generation)
would also act as the "permanent mark" from the DFS topological sort.
A value in the other range would act as the "temporary mark".

We might not need to store the output "topologically-sorted" graph, because
we're using generation numbers instead anyway.

We can assign generations from the lowest to highest number as we unwind the
traversal stack. If we hit a node with a permanent mark (generation number
already assigned), we use the max(target_node_generation - 1,
currently_planned_generation) for the planned generation number.

Now, regarding WCC IDs ("component" numbers).

Aside:
    BTW, can we somehow embed them into the generation number? Probably not,
    as they would have uneven space for their generation numbers, and we'll
    have to move them around constantly.

So, let's say we start with a WCC ID 0. For each new DFS we increment it and
assign the result to the nodes we're traversing which don't have an ID
assigned (are zero). Upon encountering a non-zero ID, add it to a list of IDs
belonging
---

Create an array of node indices able to accommodate all nodes (maybe minus
one?). Use it as a stack to remember the nodes to go back to during the DFS.

Store the number of already traversed outgoing edges in the node's
"generation".

Store the node's generation in "generation", once all edges are traversed and
we are returning to the source node.

Use two distinct ranges of "generation" to store traversal state and the
generation itself. Both excluding zero.

Thus a value in the second range (the actual generation)
would also act as the "permanent mark" from the DFS topological sort.
A value in the other range would act as the "temporary mark".

We might not need to store the output "topologically-sorted" graph, because
we're using generation numbers instead anyway.

We can assign generations from the lowest to highest number as we unwind the
traversal stack. If we hit a node with a permanent mark (generation number
already assigned), we use the max(target_node_generation - 1,
currently_planned_generation) for the planned generation number.

Now, regarding WCC IDs ("component" numbers).

Aside:
    BTW, can we somehow embed them into the generation number? Probably not,
    as they would have uneven space for their generation numbers, and we'll
    have to move them around constantly.

So, let's say we start with a WCC ID 0. For each new DFS we increment it and
assign the result to the nodes we're traversing which don't have an ID
assigned (are zero). Upon encountering a non-zero ID, add it to a list of IDs
belonging

OK, no.

How about we have an array of WCC IDs, one for each node, with the same order
as the nodes? As we do our generation number-assignment DFS, we can assign the
same WCC ID to nodes traversed in a single DFS.

No, wait.

How about we have a linked list with one element per node, with elements
stored in the same order. Then, as we do a single DFS, we keep adding new
nodes to the end of the list. If we hit an already-traversed node,

----

OK, scratch all that. ChatGPT says we can use an (iterative) LPA (Label
Propagation Algorithm) for this. Kinda what I was thinking about before.
However, he's not clear about handling edge directions there. From its
description it won't work.

How about this:
* We start with a WCC ID 0
* For each node in the graph
*   If we haven't traversed this node before
*       Increment the WCC ID
*       Start a DFS supplying it with the WCC ID, which:
*           Assigns this ID to every node without one.
*

Actually, after talking to Panos, I think we could go through finding the DAG
roots by calculating the in-degree of all nodes, storing that in the
"component" members. Then go over every node and do DFS from each that has
zero in-degree, giving it the next component ID to put into all the
"component" members, and assigning the generation number, as well as detecting
cycles. If we find no nodes with zero in-degree, that means there is a cycle
in the graph.

Now, which would be the best state to do it in? Indexed, I guess.
---
TODO: Consider storing node hashes in a separate array, avoiding the hassle of
      using flexible array members.

---

No, there's really no difference between finding root nodes first and not
finding them, for identifying WCCs.

We can always invert the graph and walk the pair of them as an undirected
graph to identify the WCCs.

But let's try something else for a bit longer. We could save on memory for the
normal case, where there are not many WCCs in each created bundle/file.

So, again, say we start with a WCC ID 0
We create an empty array of connected WCC IDs, where each ID's
entry is at the index of its ID-1, and stores the node the WCC was started at,
as well as the ID of the next connected WCC, or zero, if none.
    For each node in the graph
        If the node's component is zero
            Increment WCC ID
            Add an entry to WCC ID list with the next ID set to zero,
            and the current node as the start one.
            Do a DFS assigning the WCC ID to each node with zero component
            (as well as assigning generation numbers)
            If a node with non-zero component is encountered during that:
                If its next WCC ID is not this WCC ID already:
                    Take its next WCC ID and put it into this ID's entry as the
                    next one.
                    Assign this WCC ID into the found entry as the next one.
    Set current WCC ID to zero
    For each WCC ID array element:
        If its next WCC ID is not zero:
            Set current WCC ID to its WCC ID
            For each item in the linked list starting from the next one
                Do a DFS from the starting node, for all nodes with the same
                WCC ID set the current WCC ID

The worst case for the memory use is when e.g. every node we traverse points
back to at least one of the previous nodes. For that case we'll need the WCC
ID array length to be the same as the number of nodes. Each item in that array
being 64 bits.

---
Can we avoid allocating the WCC ID linked list, if we simply repeatedly walk
the graph?

Like, take a node, walk down the whole reachable subgraph, find the
maximum/minimum WCC ID among the nodes, then walk it again to assign that
maximum/minimum to everything.

---

OK, let's do it simple for now, just invert the graph.
TODO: Consider renaming `extra_edges` to `target_indexes`.
---
generation == 0 -> no mark
0 < generation <= 0x7fffffff -> generation (permanent mark)
0x7fffffff < generation <= 0xffffffff -> next edge (temporary mark)
component -> the node to return to
---
TODO: Test enumerating with extra edges
    - DONE!
TODO: Support "hashless" bundles - ones with hash_len == 0
    - DONE!
---
OK, it seems like we don't need to do the Bloom filter thing, because Git only
uses it for finding modified paths, not actual commits.

They're also using commit graph chains to split down files. However for us it
might not work, because we can be adding earlier commits than already exist in
the database. For that matter we need to start our generation numbers from
INT32_MAX by default, so we can grow them *both* ways: up and down.

TODO: Add file hash?

So, we would need to leave outer nodes unknown, and have a way to look up
which file they're in.

What should we be able to do:
* Lookup a commit by hash
* Follow the edges to the right file and commit

Git doesn't have anything helping find the right file, except the fanout
structure.
---
TODO: Unify plural usage in compound names (nodes_num vs node_num, and so on).
TODO: Make kcidb_bundle_compact() react to memory allocation failures.
    - DONE!
---
TODO: Move hdag_node_hash_*fill* to hash.h, or even to misc.h
    - Ah, not needed.
TODO: Use hdag_node_off_const wherever applicable
    - DONE!
TODO: Make HDAG_BUNDLE_IS_SORTED_AS a function
    - DONE!
TODO: Make darr members and functions match verb/noun order (slots_occupied vs
      occupied_slots)
---
OK, each file can contain multiple components and they can be from any part of
the graph. So we need to maintain a mapping from local component and
generation numbers to global numbers, so we can meaningfully compare nodes
from different files.

It would also be good to maintain a mapping of "unknown nodes" to their
location in the files where they're "known", to speed up lookups. However,
that's not strictly necessary for the start. The paragraph above is more
important.
---
No, we can't really support adding child nodes and not rebuild parent files.

We must assume that the generation numbers are consistent throughout, and
component numbers match, and only keep additions of parent nodes in separate
files. Otherwise we have to rebuild existing parents when adding children.
---
OK, after thinking about it for a while, it look like we'll need file-global
node indexes after all, as well as global generations and component IDs.
To be able to do that, we'll have to recount all files with unknown nodes
appearing in a newly-added file.

Each file would have to be given the starting index for its nodes, so that it
can offset the stored numbers. Each file would need to be given access to a
function looking up a node outside it, so that generations and components
could be calculated correctly.
---
We have to distinguish the indexes inside the bundle's nodes array, and the
global indexes, referencing a node in the bundle's (arbitrary) context.
The translation between them is done using the `nodes_off` structure member.

Should we call them local index and global index?

Ah, wait we gotta distinguish the local index that is inside the nodes array,
and the global index that points to this bundle (is within its range,
considering nodes_off and the size of the nodes array).

Could we call the global index "node index", and the local index the "array
index"?
---
OK, we have the convention of "lidx" and "gidx" for local and global indices
respectively. Now, what if we wanted to access nodes in a bundle both by local
and by global indices? How would we distinguish and call such
macros/functions? "HDAG_BUNDLE_GNODE" and "HDAG_BUNDLE_LNODE"?

Or should we simply make all bundle node accesses use local indices?
---
OK, no having global indexes requires us to manage the index space and
reshuffle them when earlier nodes are added. Git doesn't have to do that, so
they can afford global indexes. We'll have to have looser coupling.
We can have global component IDs, but local indexes, and not refer to outside
nodes using indexes, but using hashes, and look them up on traversal.

This would also help modularity and isolation.

This also means that multiple files can contain an unknown node, and we won't
know it's unknown until we find it. OTOH, we would still have to look through
all files even if they don't have that node at all, until we stumble on the
one with the actual node.

Should we then simply not store unknown nodes? Ah, wait we have to, because we
have direct index system. OK. Anyway, this is a small matter and doesn't
affect correctness, and this is what we're after first of all for now.
---
OK, we gotta decide what we can do with duplicate nodes and edges.

When we read in a bundle, we add both the nodes themselves and their
references to the nodes array. Then we pick a node in favor of references, and
we leave just one reference, if there are no actual nodes. What should we do
if we have multiple nodes?

Our choices:
    * Simply drop the duplicate without inspecting it
        * When processing bundle, drop the node completely
    * Report an error without inspecting the duplicate
    * Make sure the duplicate is the same as what we already have.
        * If yes, drop it.
        * If not, report an error.

Actually there are two dimensions: how to match the nodes:

* Just the hash
* The hash and the targets

and what to do on matches:

* Drop the new duplicate
* Report an error

What about edges? Edges are defined by the nodes they connect. There's nothing
to compare. If they're duplicates, then they're complete duplicates (whereas
nodes can have a different set of out edges).

Our choices:

    * Report an error
    * Discard duplicates
    * Keep duplicates
        * Is this even possible? Should be? We can debug it later.

In addition to that we need to check the context for pre-existence of nodes.
We don't need to check the context for pre-existence of edges, because we
wouldn't have the same node in our file to create a duplicate edge.

---
So what could be the operations for checking node presence in a context?
---
So, when we scan the bundle after sorting, and encounter a known node, we need
to check both if the bundle and if the context already have it. We can do the
context check after we've finished the bundle's run check, and decide to
either drop the node, or raise an error, if it's a duplicate.

---
OK, how about the context node-finding interface?

targets specified:
    none    - node not found
    hash    - conflict          - error reported
    full    - completely identical node found - new duplicate discarded or error reported

targets NOT specified:
    none    - node not found
    hash    - node found - new duplicate discarded or error reported
    full    - not possible

So we have two booleans:

compare targets: true/false
discard duplicates: true/false (report errors)
---
TODO: Rename match_targets to something more general, like "match_contents".
      - done
TODO: Rename "drop_dups" to something else which would mean "ignore
      duplicates" when set to the default false.
      - done
---
So, we need to deal with duplicates in the added data, and between the
existing and added data. We also need to link up the edges, and update
components and generation numbers.

So we've added "contexts". But perhaps we shouldn't have. How about we filter
the node sequence we're feeding to the bundle, and then map the component
names? Hmmm, no, we won't be able to calculate the generation numbers properly
without referring to the nodes outside.

For that matter we should recount generations and components of all upstream
files too.

And that means we should be able to find all linked files quickly.

Given that we can have completely independent components in each file, two
files can be simultaneously upstream and downstream from each other, leading
to a dependency loop. That also means that we cannot represent connections
between them via an HDAG file in turn.

Perhaps we need to restrict files to one component only?

Or perhaps we could simply mark each affected file, avoid marking them twice,
and append them to the sequence being consumed?

To be able to quickly decide whether a particular file refers to our
file-in-the-making, we would need a separate index of "unknown nodes".
So we can quickly find them. Perhaps just a sorted list of hashes?

For now, just to verify how everything works, and move ahead, we could simply
search the main node list for all unknown nodes, and check which ones exist in
the node sequence. Ah, that's gonna be pretty slow, though. Maybe we should
add a sorted list then.

So, for each node we're feeding to the bundle being created, we search all
unknown nodes in all the files. We mark files for addition to the bundle once
we find they have an unknown node that the bundle has. We don't look at the
files again after they're marked. Once we processed all the original nodes to
be put into the ... wait.

How about we first put the incoming nodes into a bundle within our context, so we
could look up its nodes faster? Then we go over all unknown nodes, and if we
don't find any of them in the new bundle, we just add it to the database as a
file. We add the files with unknown nodes referring to the files we already
added as well as the new bundle.

Perhaps we can do half of the processing there, like only up to filling the
fanout array, and not assigning components/generations? To just make it easier
to find unknown nodes there?

Then we can put this bundle and the marked files into a node sequence, and
load them into the final bundle, with generations and components enumerated.

Actually, the bundle asks the context if it can add *every* node, but *only*
after the node sequence is finished and it's deduplicating them. So we need to
decide what to do when giving it the nodes instead.

The problem is that we would be deduplicating twice. First the new bundle
alone (or in context), and then again together with the affected files.
Perhaps we should do it only once. Say, when feeding the nodes we could be
scanning if each node is unknown somewhere. And then adding that file to the
end.

Or perhaps we should just make that bundle. And actually add generations and
components, because that checks for cycles.

We would also need to do that in context.

Redoing the context: it could just give us abstract info on a particular node.

Where would we put the match_content and reject_dups flags, though? It kinda
makes sense to keep it in the context too, as the context, being a container
of the nodes has to maintain certain integrity requirements. Well, that only
concerns the "match_content" option. Reject dups is an ingestion option, and
mostly concerns input data. Also, ideally we want to compare contents
normally, and disabling that is an optimization.

So, how about we enable checking contents forever, and quietly drop all the
dups all the time? This way we would have nothing to configure for now, and
would make things simpler until we actually have a need for it.
---
we cannot return a hash sequence, because we don't known the size of the
private sequence data.

what if we just copy targets to a (sorted) dynamic array that the user
provides us with?

---
ok, we can say that each context type has its own structure, which includes
the original, abstract context structure. this way we can have fixed extra
data for the context. however, we still would need to return an arbitrary
number of hashes. perhaps we can embed the state of target hash sequence in
there as well.
---
Removing 'const' from sequence parameters.
    - DONE!
Switch to passing sequence pointers around
    - DONE!
Add HDAG_CONTAINER_OF (test it!)
    - DONE!
Support resetting node sequences
    - DONE!
Add next/reset accessors to node sequence
    - DONE!
Fix some misspelling in the docs
    - DONE!
Use HDAG_CONTAINER_OF
    - DONE!
Add resetting to bundle's node sequence
    - DONE!
DO NOT remove context from deduplication
    - DONE!
Add hash array sequence
    - DONE!
Add context to enumeration
    - DONE!
TODO: Use designated initializers in *_seq_init() functions
    - DONE!
TODO: Remove _fn from function defs
    - DONE!
TODO: Remove the *data member from all sequences, and use HDAG_CONTAINER_OF() instead
    - DONE!
TODO: Make sure we're using sequence accessors everywhere, and not member functions directly
    - DONE! None uses were found!
TODO: Use "base" not "seq" in hash array sequence
    - DONE!
TODO: Return pointer to a hash from sequences, do not copy it.
---
We need to remove duplicates in the added bundle itself.
We need to remove bundle nodes which already exist in the database
We need to calculate generation numbers in the added nodes
We need to calculate component numbers in the added nodes
We need to recalculate component and generation numbers in files with children
nodes (and their children files).

OK, here's the plan:

    Create a bundle from the node sequence without a context
    Lock the database for writing
    Create a list of indices of database files (file_idx_list)
    Set number of recalculated files (recalc_num) to zero
    For each known node in the created bundle:
        Remember recalc_num
        For each file indexed by file_idx_list
            If a node with the same hash exists in the file:
                If it's unknown:
                    If the current index is at recalc_num:
                        Increment recalc_num
                    If the current index is above recalc_num:
                        Swap the current index with the one at recalc_num
                        Increment recalc_num
                Else it's known:
                    If the node contents mismatch:
                        Raise an error
                    Mark the node unknown in the bundle
                    Restore recalc_num
                    Break out of the inner loop

    Do:
        Remember recalc_num
        For each file indexed by file_idx_list at or above recalc_num:
            If any of its unknown nodes are known by files under recalc_num:
                If the current index is at recalc_num:
                    Increment recalc_num
                If the current index is above recalc_num:
                    Swap the current index with the one at recalc_num
                    Increment recalc_num
    while recalc_num is changed from the remembered one

    Create a sequence returning the bundle nodes, followed by the nodes from
        the files under recalc_num
    Create a bundle from that sequence with the database as the context
    Create a file from that node
    Lock the database for reading
    Add the new file
    Remove the old files
    Notify other processes of database change
    Unlock database for reading
    Unlock database for writing
---
TODO: Consider using 'len' and 'space' instead of 'occupied_slots' and
      'allocated_slots' in darr
TODO: Get rid of the need to use ssize_t in darr loops
TODO: Make separate macros for darr loops with init and incr expressions
TODO: Make nodes.h use size_t instead of uint32_t.
---
We gotta consider implementing support for presenting a file as an
unresizeable bundle to avoid reimplementing operations.
    - DONE, implemented!
---
Actually, perhaps we could make bundles serve files as well...hmm...

So we could have "bundles", which are "organized" and then "filed".

And then we could create a bundle from a file, which would mean memory-mapping
that file and linking the fields. BTW, we might need to turn the fanout array
into a dynamic array, so we could more conveniently link it into the
memory-mapped file.

Then we could also support only a limited number of operations on a file, e.g.
"forget_node" only.

TODO: Generate a new "unknown_hashes" array in one go after removing all the
nodes, when merging a new bundle.
    - done!
TODO: Check that we're using HDAG_DARR_ELEMENT macro everywhere we can.
---
NOTE: Fanout: empty is zero, but zero is not necessarily empty.
              Or should we have them completely separated?
---
Actually, we cannot simply remove a node, because that potentially requires
re-enumerating generations and components, so we have to create a new
bundle/file, anyway.
---
TODO: Check if hdag_res return value is marked with [[nodiscard]] in the CI
---
So, if a file is a part of a bundle, should we make it possible to create a
bundle from a free-standing file?

Or should we instead make bundle *refer* to a file? But if we do that, it
would mean that destroying a bundle wouldn't close a file. Which is probably,
OK, but it would also mean that the database has to maintain the list of files
and close them separately. OTOH, it has to manage the directory with the files
and delete and rename them.

So, suppose we have the file state integrated. Then we can move the contents
to a specified file and link it. We can unlink the file and move the contents
back, and close the file. We can't rename or move the file, because we don't
know its context.

OK, how about we separate file opening and closing and moving contents?


open
close

    BUNDLE  HANDLE  FILE    OP      BUNDLE  HANDLE  FILE    COMMENT

    empty   closed          create  empty   open    empty
    empty   closed  empty   open    empty   open    empty
    empty   open    empty   close   empty   closed  empty
    DAG_B   closed  DAG_F   open    DAG_B   open    DAG_F
    DAG_B   open    DAG_F   write  [DAG_B]  open    DAG_B
    DAG_B   open    DAG_F   read   [DAG_F]  open    DAG_F
   [DAG_F]  open    DAG_F   write  [DAG_F]  open    DAG_F   NOOP
   [DAG_F]  open    DAG_F   close   DAG_F   closed  DAG_F   DAG_F copied
   [DAG_F]  open    DAG_F   open   [DAG_F2] open    DAG_F2  implied close

                            link
   [DAG_F]  open    DAG_F   unlink

OK, suppose we weren't embedding the file, but only linked it. Hmm, then we
wouldn't have control on its closing and opening. Which doesn't sound great.

So, OK, once again. Suppose we can be told to close or open the file. But then
we can require that the bundle's data be unlinked from the file, either by
copying the file's contents, or by emptying the in-memory contents.

Let's say we can have two states: linked ("filed") and unlinked ("unfiled").
The former has dynamic arrays pointing to the file contents, and the latter
has them pointing somewhere else (or being empty).

The "filed" state requires an open file. The "unfiled" state doesn't care?
Actually, can we equate "filed" with an open file, and "unfiled" with a closed
file, after all?


Actually we can have two variants of both "filing" and "unfiling":

* create file and move bundle contents there, linking it
* discard bundle contents, open file and link its contents to the bundle
* move file contents to the bundle and close it
* empty the bundle and close the file

Maybe we can come up with four verbs for these?

* "save"
* "load"
* "detach"
* "abandon"

---

Aargh, it still feels artificial. OK, let's try to break down the operations
again:

We can't really create a file without contents so we can only create the file
from bundle contents

But we can open a file and have the bundle's contents separate from that.

Yet, we can't really put the bundle contents into an existing file.

So when opening a file we can only link its contents to the file.
Can we call it "open_file"?

And when creating a file we can only put the bundle's contents in there.
Can we call it "create_file"?

Both operations create a linked bundle.

And we can e.g. say that only a linked bundle can have the file open.
So we can have two states: "filed" and "unfiled". "Filed" meaning we have the
file opened and linked, and "unfiled" - the opposite.

Another operation could be to create a file with the bundle's contents, but
not link it and not free its memory. But that operation can skip using the
embedded struct hdag_file. Call it "save_file"?

Once we have a linked bundle we can close the file and then we can decide
whether to empty the bundle, or move the file's contents to the bundle memory.

Can we call the former "close_file" and the latter "unfile"?
Actually, essentialy "close_file" would be equivalent to "cleanup" (which
would close the file, and unfortunaterly would be able to fail). So we
don't need a separate operation name.

And then might want to just open a file, copy its contents into memory, and
close the file in one go. Can we call it "load_file"?

OK, let's try to put it together:

hdag_bundle_is_filed
hdag_bundle_is_unfiled
hdag_bundle_file_in
hdag_bundle_file_out
hdag_bundle_unfile
hdag_bundle_save
hdag_bundle_load
---
Wait, what if we have the "hdag_bundle_from_file" operation, which creates a
bundle with the file taken ower, and linked in, and then expect the user to
deal with file creation or opening?

What if we make it easier to create a file from a bundle by implementing, say,
"hdag_bundle_save". And have "hdag_bundle_load" do "hdag_bundle_from_file" and
then "hdag_bundle_unfile"?

So, how about this:
hdag_bundle_from_file
hdag_bundle_save
hdag_bundle_load
hdag_bundle_unfile
---
Read text into a bundle
Save it to a file
Create a bundle from the file
Output the bundle into text
Compare to the input text

---
For each node sequence
    Read the node sequence into a bundle
    Save the bundle to a file
    Create a bundle from the file
    (or maybe do "file" instead of the two above)
    Compare its node sequence to the original sequence
---
Consider replacing "node sequence" with just "hash sequence sequence".
---
So, how do we deal with "cleaning up" the bundle?

We can say that "cleaning up" always doesn't care how the cleanup went: we're
just releasing the resources, and then we add an additional function which
makes sure the data is preserved.

We actually doing the syncing when creating the file, so it should not fail.
---
Add a run of tests under valgrind to the CI.
    - DONE
TODO: Sort out the sequence initializers, so they're safer and less confusing
      You can't really initialize an abstract sequence with them as the
      derived one will get discarded. It only works for passing arguments.
---
cleanup

    delete file
     /      \
unfile     cleanup

      unfile
      /    \
delete      close


close     delete
    \     /
    cleanup


    filed -> unlink -> clean
    filed -> unfile -> unfiled -> unlink

Should we separate having an opened file embedded and the contents
linked to it?

We would still not be able to have the contents linked to an embedded file,
and have it closed at the same time.

Ah, OK, we can actually unlink the file without closing it or unfiling our
contents.

We can e.g. unlink the file first, and then cleanup the bundle. At that moment
it will be closed, and we were the last ones to close it, it will actually get
deleted. This is what we seem to need.

In a way unlinking is similar to renaming, only the new name is NULL.

So, the generations in the bundle being added will be calculated correctly,
without rebuilding the child bundles. However, component numbers might not be
correct, because the new bundle might have connected two previously-separate
components and now they need to have the same ID.

Wait a minute, that means that we would need to recalculate component IDs in
parent bundles as well... oh, man...

To avoid that we would need to maintain the component number mapping after
all. Can we be smarter somehow, and avoid both that and rebuilding parent
bundles?

OK, we can use Union-Find system to maintain component connectivity info.
But now we would have to maintain global component IDs.

Perhaps that should be the job of the context?

And while we're at it we should rename "context" to just abstract HDAG
providing basic functions only? And derive both the bundle and the database
from it?

Generations are assigned from parents to children, starting with one.
When creating the bundle with the new nodes/edges, the generations would be
assigned correctly, and won't be changed by rebuilding the child
components/bundles.

----

So, the problem with components is that we need to traverse the whole DAG
*both* ways to identify them. And so, a new bundle could be connecting two
previously-disconnected components in parent bundle(s). Hmmm, perhaps it would
be easy enough to copy the affected bundles, and update their component
numbers? This wouldn't change their sizes. It would need the disk space, of
course. And would need a lot of blocks written, if the component being updated
is a major one. OTOH, it would make things *much* simpler to handle.

How would we find the bundles to rebuild and the component IDs to update?

OK, let's suppose we're building the merged bundle, and traverse its nodes to
identify the components. We stumble on an unknown node which we can
dereference through the context and get its component ID. By that time we've
already traversed a part of the component and assigned it a new number. We
would have to re-traverse that part to correct its ID, which wouldn't be fun
or all that performant. Plus, we might hit another unknown node dereferencing
to yet *another* component ID later, and would have to redo this again. If
we're to rewrite component numbers, it should be done *after* we've fully
traversed all of them.

Or we could follow the old idea of maintaining a mapping between the component
IDs, using the "union-find". We should be able to avoid storing it on disc,
and reconstruct it on each start up from the "unknown hashes", or perhaps
better "unknown nodes" arrays. We could realign the component IDs whenever
we're rebuilding the bundles. BTW, rebuilding any bundles would mean we would
have to rebuild the "union-find" mapping too. In all processes. Having the
component IDs rewritten would rid us of this hassle and complication.

We would still need the context to give us the new component IDs, though.
And if we simply give a new one each time, and we have a lot of additions, we
could run out of them eventually. We'll need to reclaim them somehow.
*And* we'll need to store the next available one somewhere too. Hmm...

In any case, bundle component enumeration wouldn't need to take the component
IDs of dereferenced unknown nodes into account at all. So we can simplify the
context interface to remove that. It would still need to reference the
generation numbers, though.

So, since those component ID mismatches wouldn't be discovered from the POV of
a bundle, we can comfortably do it from the database's POV.

We can go over all the "unknown" hashes of the merged bundle, and collect the
mappings into e.g. the same "union-find" structure. Then we can go over each
set and update every bundle in the set except the biggest one, for example.

OTOH, if we have the union-find structure, we can manage the ID distribution
and find new untaken IDs, if necessary.

Hmm, we wouldn't really need to have all the IDs in the union-find, only those
spanning multiple bundles. And it would be easier to have only those there,
otherwise we would need to traverse all the nodes in all the bundles to find
all of them. If we don't have all the IDs in the union-find, then we can't
reasonably give out new ones. So that means we would have to store *something*
specifying which IDs are available and which are not. Reconstructing the
complete structure every time we start (and rebuild bundles!) would be too
costly, for just finding the next free ID. Perhaps each bundle could keep a
set of its component IDs stored.

OK, how about we let the bundles manage the component IDs independently? Only
keep generations influenced? And then we map them into the global ID space
somehow in the DB? Hmmm.... Perhaps it would be similar to keeping the
information about used IDs inside the bundles somehow. Let's say, if they're
contiguous, we could just keep the number of IDs there?

It looks like they are! So, we can just have the number of components there.

Oh, and BTW, we can keep an array of indexes of unknown nodes, instead of
unknown hashes. This way we can quickly get to their component IDs and avoid
duplicating more of their data.

OK, so how would we map the component IDs from each of the bundles into a
single contiguous space?

123
   12345678
           12345
                1
                 123

We should be able to quickly say if a component from one bundle is connected
to a component in another bundle.

So, we find a node and get its bundle and component IDs. We find another node
and get the same.

If we have an array of accumulated components, similar to the fanout array for
the bundles, we would be able to quickly find their entry in the union-find.

OK, let's go with this:
* Have a "fanout" array for quickly finding a component entry. Or we can even
  skip that and simply add the component numbers as we're looking up.
* Have a component entry array, where each bundle's components are represented
  in order, by the following structure:
  * Bundle index
  * Component index
  * "Parent" component index in the same array
  * "Rank" or "size"

Wait, how would we be finding cycles between components with this? We would
have to consider the connecting node directions, which means we would have to
go and check the unknown node entries in both bundles.

Could we somehow employ a bundle for this, as we've considered a long time
ago? We could represent each component as a node. And then represent each
unknown->known node connection as a directed edge. We would get cycle
detection for "free". Each bundle-spanning component would actually get its
own component ID.

Now, how would we for example check if a node connected to another node using
this component bundle? We could look up the first node and find its bundle.
We could then try to look up the other node in the same bundle. If it's not
there, we could try to find the node's component in the component bundle.
Let's assume we have every component there, and we can follow the edges to
other bundles until we checked all of them. If we didn't find the other node in any,
then we check if any of the checked bundles had any ultimately-unknown nodes,
and if they did, we can say we don't know if the nodes are connected, because
we don't know if the other node exists. If none did, then we can say the node
does not exist.

Eh, no, we gotta look up both nodes first, then get their components, and then
check the component bundle to see if their components are connected. Well,
first we would check if they're in the same bundle, of course.
---
What if we hash the bundle/component IDs, and have a map of
hash->bundle/component

Oooor, what if we put a hash of bundle/component into the node hash, followed
by the actual bundle index and component index?

Yep, it looks like we could use something like splitmix64 and cram 32 bits of
bundle index and 32 bits of component ID in there. So we would have 128 bit
hash in the component bundle.

And we won't need to know the number of components in each bundle!
---
bundle pointer list
bundle pointer number
current source bundle pointer index
current unknown node index index
current component (of the source bundle)
current pointer to the known target node


For each bundle
    Set "component" to zero
    For each unknown node index
        Get the unknown node at the index
        For each other bundle
            If it contains the node with the unknown hash as "known"
                If the unknown node's component is not the same as "component"
                    Generate the source node hash from the outer bundle index and
                    the unknown node's component ID
                    Output the source node
                    Set the "component" to the unknown node's component
                Generate the hash of the target node from the inner bundle
                index and the known node's component ID
                Output the target node
                break


targets_hash_seq_next
    if there's no last found target
        return end-of-sequence
    output last found target
    try to find the next one

Get next
---
So, we want to find the next

OK, no, we cannot have node index in the components hashes, because we want to
have a single node

Wait, OK, a component piece might be referencing multiple other pieces in
other bundles. That is done through multiple unknown nodes in the source
bundle an one or more known nodes in the target bundle.

So, a single component piece in one bundle might be referencing a single
component piece in another bundle via multiple nodes.

Actually a single component piece in one bundle might be referencing multiple
component pieces in another bundle.

OK, how about we call them "local" and "global" components? Or "bundle" and
"db" components?

So, a node in the "components" graph should correspond to what? To a local
component? Seems right. Then we cannot have node IDs in the hashes, because
there could be multiple referenced/ing nodes per local component.
---
So, we not only need to deduplicate the source component ID, but also the
target component ID. And while we can sort the unknown nodes by component ID,
we can't really sort the target nodes like that. Oh, wait, no, we can sort the
unknown nodes by source component ID first, and by target component ID second.

But that would mean that the bundle itself cannot do that, because other
bundles don't really exist for it. Or do they? It has the context object it
uses to retrieve generation numbers, why can't it retrieve component IDs for
sorting?

So, we need to go towards one of the options: either isolate bundles as much
as possible (including enumerating generations), and move gluing them together
to the database, or use the context fully and lean on it, dropping local work
making bundles easier to glue together in the DB.

Let's try to think about isolating generation enumeration. Actually, if we
take the maximum generation among all downstream components of a particular
component, then we can use it as an offset for all generation numbers of that
component. The only problem might be exhausting the available generation
numbers, but this number would still be less (or equal) to the total number of
nodes (a hunch). Then, we gotta store those offset numbers somewhere, and we
actually can store it in the component piece graph, in the generation number,
because logically they would align in ordering, and would be correct, even if
not contiguous.

Now, given that, we would be able to get rid of the context, wouldn't we?
Seems like it!

We would have to go over the component piece graph, though, to recalculate the
generation numbers.

And going back, we still have to generate the nodes for the component piece
graph. And so, we would have to take the unknown node sorting out of the
bundle.

If we could it would be nice to support creating bundles from the edge list,
not just adjacency list (node sequence). Because that's the natural output of
going through the unknown nodes. Perhaps a solution for that would be
accepting duplicate node hashes with different target hashes, and instead of
aborting, merging them, in the bundle.
---
We gotta remember the index of the first node in a span,
then when the span ends, we sort and dedup the hashes, and assign each node in
the span the same 

Ah, wait, we're reducing multiple 

Wait, first, we won't need the separate edge deduping step.

We're reducing multiple nodes to one. And all nodes except the first one will
be discarded.

Aaactually, we would need to discard a bunch of duplicated targets, and would
probably need another array of target hashes allocated.

So, when a span of same-hash nodes ends, we go over each node in order and
pick the smallest hash. Ah, wait, that's too many comparisons.

So, what if we rearrange and sort the targets at the same time?

Like, we find same-hash nodes, and then go over each target taking the
smallest target hash larger than the previous one, and adding it at the new
location, and then we assign the same span of target hashes to all the
same-hash nodes?

Wouldn't that be faster?
Let's see.

last_min_target = NULL
while true
    min_target = NULL
    for each node in the span
        for each target of the node:
            if (last_min_target == NULL ||
                target->hash > last_min_target->hash) &&
               (min_target == NULL ||
                target->hash < min_target->hash):
                min_target = target
    if min_target == NULL:
        break
    append min_target->hash to the new target_hashes
    last_min_target = min_target

So, basically we need to compare two hashes this many times in the worst case:
    total_targets^2 * 2
---
So we would be better off with concat->qsort->dedup.

Now, technically, we could keep sorting and deduping separated in two API
calls, but it looks like we can benefit from doing it in a single call, now
that we've decided to ignore duplicates and merge targets.
---
TODO: Implement a convention for naming function with "slice" if it works on a
      slice of an array, and without "slice" if it works on a complete array.
---
    if told to merge targets
        for each node duplicate
            append targets to the output hashes
        sort combined targets
        dedup combined targets
        update the first known node with the range of targets
    else
        for each node duplicate
            sort targets
            if it's not first node in the run
                if this node's targets are not equal to the last node's
                    report conflict
    copy the first known node to the output pointer
    increment output node pointer


if told to merge targets
else

---
TODO: Use uintptr_t instead of passing pointer to a size, for comparison
      functions.
TODO: Consider using size_t as the hdag_hash_len_is_valid() parameter.
---
OK, so right now we're trying to only return const elements/slots out of
a const struct hdag_arr. But these are not really equivalent. We can be given
a const struct for creating a slice of an array out of, so we don't modify the
original, but we should be able to modify the elements of the new slice.

So, how about we switch to checking the "constant" flag instead?
---
We want to be able to create an empty bundle node sequence without specifying
a bundle, and thus a hash length.
