OK, database location is specified as a directory path, where the library
controls the files for whatever needs it has.

---
Do we need any metadata assigned to edges?

We need to express that this revision is a child of another revision, because
it's a new revision of a patchset. OTOH, the fact that a revision's edge
target is a patchset says that this revision is a new version of one. No
matter if this revision is a git commit or a patchset itself.

So, let's say we have a series of patchset versions each based on a particular
commit, which then gets merged as the following commit, with a few commits on
top:

[a]
 ^
[b]
 ^
[c] <- (1)
 ^      ^
 |-----(2)
 |      ^
 |-----(3)
 |      ^
 |-----(4)
 |      ^
 `-----[d]
        ^
       [e]
        ^
       [f]

What if the patchset version (3) fixes an issue which was introduced in
commit [b]?

In the absence of other information we're supposed to say that the branch has
the issue fixed now.

However, if we travel down both edges from [d], one will contain the issue,
and another would not. At the same time, the fix would be the same distance
from it as the issue itself. What should we say? Is the issue there, or not?

We could say that since there's a commit without the issue *after* the one
with it, it's *probably* fixed. So the test would be get all closest mentions
of issues, and if there are any ambiguities, check which one is before which.

If we can't "see" the issue from where we are (i.e. a revision with it
reproducing is masked by another with it *not* reproducing), then we can say
the issue is *likely* fixed.

We can only say that the issue is "definitely" fixed if we tested the
particular commit.

Here's a more involved history:


[a]
 ^
[b] <- (1)
 ^      ^
[c]     |
 ^      |
[d] <- (2)
 ^      ^
[e]     |
 ^      |
[f] <- (3)
 ^      ^
[g]     |
 ^      |
[h] ----'
 ^
[i]

So, for [i], if an issue is detected in [e], and not in (2), then it's still
there.

Or, if an issue is detected in [e], but not (3), then it's probably not there.

Then, if an issue is detected in (3), but not in [h], then it's likely not
there.

Finally if an issue is detected in [a], but not in [i] itself, then it's
definitely not there.

In all these cases, we don't actually need to know if a revision is a commit
or a patchset.

What about merge requests getting merged?

Can we record them like this:

[a]
 ^
[b] <- [A1] < [B1] < [C1]
 ^                    ^
[c]                   |
 ^                    |
[d] <- [A2] < [B2] < [C2]
 ^                    ^
[e]                   |
 ^                    |
[f]                   |
 ^                    |
[g] <- [A3] < [B3] < [C3]
 ^                    ^
[h]                   |
 ^                    |
[i] ------------------'
 ^
[j]

Well, first of all the merge requests consist of actual commits themselves,
and then they can be actually merged, or simply rebased and fast-forwarded to.

In this case, if [B2] was found to fix an issue, then we can assume that [C3]
has the fix (but note, not B3, with this model), and therefore [i] can be
assumed to contain the fix as well.

Here we have both actual merge and a succession of the merge request version
expressed with the same type of edges, and we have no way of distinguishing
them.

Should we introduce edge types, and if yes, how are we going to use them in
queries? Bitmasks? Numbers?

Should we have more edge types? E.g. between releases? The latter, probably
not, as they're connected after all.

Nooooo... This would require us to build separate generation numbers, and
bloom filters for different sets of edges. So perhaps we can mark the
revisions somehow instead? E.g. say if both the parent and the child are merge
request heads, then the edge is an MR version update? Hmm, that's actually not
necessarily true. What if an MR version was fast-forwarded to, then the next
MR was a single commit, which got fast-forwarded to as well? What would the
edge between them signify? Ah, they would have to have the same MR IDs
assigned to version heads to qualify as versions of the same MR.

So, no we won't have different types of edges. That would be too much.

I wonder if we could simply have separate graphs for separate edge types?
Hmm, that could do it!
---
OK, how do we fill a newly-created HDAG file with data?

We can supply it at the moment of creation.
Afterwards we don't allow changing it, except deleting nodes (marking them
unknown).

Perhaps the hdag_file_create can accept a callback which would return the node
data, plus a function for retrieving its outgoing edges.

We shouldn't expect to know the number of nodes from the start, because we
need a node allocated for targets as well, which might not be in the nodes (be
unknown at the moment, or be in another HDAG file?).

Aaah, and here we need to know which nodes are already present in other files
and which not. That means that the database needs to look them up through all
the HDAG files. That also means that the database can take care of sorting and
arranging the data for each file. And the hdag_file_create can accept
pre-cooked data, and can be kept simple. Or even the database code can fill in
whatever is needed itself, once the file is created. This would mean that the
file module wouldn't need to think about locking, consistency, or anything
else. Can we do that? Well, it would be good if it was the job of the file
module to take care of internal consistency, and for database module to take
care overall database consistency (no duplicates, no loops, and so on).

Although, the file module would also need to take care of loops. Does it make
easier the database's job of looking out for loops? Say, it could look for
unknown nodes in each file and see if they're present in other files. If
a node in one file refers to a node in another file, then they should have the
same component ID. That should be arranged by the database code.

We need to distinguish the components, so that we can compare generation
numbers.

Yes, the file module would need to do sorting as well, or at least verify it,
because it has to provide the interface for looking up nodes.

We don't need an online sorting algorithm when ingesting nodes, as our storage
structure cannot be reshuffled efficiently, and so we cannot use it as our
storage for sorting in progress. We would need to accumulate the received
nodes, then sort them, and then output them into our storage sorted.
---
OK, we don't know how many node entries we'll have in the file, so we need to
either store the incoming data in a separate structure, or be able to resize a
file. The problem with sorting the file structure is updating the target
references. In particular, we have to walk them back, which the file has no
optimization for.

So, perhaps we would need to sort them in a separate structure, after all.
And then we don't really need to make the file resizeable.

We can allow supplying both an adjacency list and an edge list when creating
the file, because we would have to have an edge list anyway to avoid
variable-length node records, or updating references.

Hmm, actually, perhaps each node in this representation can reference the
starting and the ending indices in a separate list of targets instead?

This way we can first fill up the unsorted node list... no...

We would have a problem with consuming a separate edge list, because we won't
know how many targets a node would have, only after we've seen the whole list,
and that would require us to reshuffle the list constantly as we're consuming
it.

So let's drop it and just consume the adjacency list.

So:
    * Fill in a (deduplicated) array of node hashes:
        * The hashes returned by node_seq
        * The hashes returned by hash_seq

    NOTE: We gotta mark hashes for which we haven't received a node yet,
          somehow

    * Fill in an array of target hashes

    * Sort the node hash array.

Actually, we *can* use struct hdag_file_node, and simply always use target
list references. But we would need a different target list from the one in the
file, as it should contain hashes, rather than indexes into the node list.

OK, which sorting approach do we take? If we use qsort, the performance should
be good, since the data is randomly distributed, but we will need to store the
list of all nodes and *all* targets, because deduplicating requires lookup.

I.e. we just store all node hashes, and all target hashes, without
deduplicating, then we sort, then we deduplicate. In general, in git history
graphs, there's not much duplication of target nodes. So it probably won't be
a big deal. Ah, wait, we would need at least 2N-1 memory to put both the
nodes and their targets in the same array, without deduplication. So, maybe we
better use skip list sort for online sorting and deduplication.

OK, another try:

    For each received node and its adjacency list
        Insert

Ah, wait, if we're going to simply put *all* the target hashes into an array,
without packing most of them into the node, then we would take a similar
amount of memory as the qsort approach does.

OK, let's start with the simplest, qsort approach.

So:

    For each received node and its adjacency list
        Remember the length of the target array
        For each hash in the adjacency list
            Add an entry into the node array:
                hash
                first target index = unknown
                second target index = unknown
            Add an entry into the target array:
                hash

        Add node info to the array:
            hash
            if current target array length == remembered length
                first target index = invalid
                second target index = invalid
            else
                first target index = remembered length
                second target index = current length - 1

    Sort node array

---

Sooo, how would we deduplicate the preprocessed array of nodes?

We need to leave only one node of each hash. After sorting they will be put
together.

If there's a node with known targets among same-hash nodes, it should be used.
Otherwise, any node with unknown targets should used.
---
Add nodes and target hashes to corresponding arrays, as they come in
Sort nodes by their hashes
De-duplicate nodes by their hashes
Compact the targets
---
# Compacting
for each node, start to end
    if node targets are unknown
        continue
    if there's more than two of known targets
        set node's first target to indirect index of next available extra edge slot
        for each target hash
            lookup node index
            add an extra edge with that target index
        set node's second target to indirect index of the last taken extra edge slot
    else
        for each target hash
            lookup node index
            store it in the node's targets as direct index

---
TODO: Rename 'edges' to 'extra_edges' in the file.
TODO: Use 'nodes_num' and 'extra_edges_num' for consistency.
TODO: Consider renaming HDAG_TARGET_INVALID to HDAG_TARGET_NONE, or
      HDAG_TARGET_ABSENT.
        - done!
TODO: Deduplicate edges.
        - done!
TODO: See if we could normalize on the "destination hash", instead of "target
      hash", or vice versa.
TODO: Consider replacing "ind_extra_edges" with looking at which of
      target_hashes/extra_edges is non-empty.
---
ASSIGNING GENERATION AND COMPONENT NUMBERS

Create an array of node indices able to accommodate all nodes (maybe minus
one?). Use it as a stack to remember the nodes to go back to during the DFS.
Actually, just resize it as necessary, in case we have a "wide" graph.

Store the number of already traversed outgoing edges in the node's
"generation".

Store the node's generation in "generation", once all edges are traversed and
we are returning to the source node.

Use two distinct ranges of "generation" to store traversal state and the
generation itself. Both excluding zero.

Thus a value in the second range (the actual generation)
would also act as the "permanent mark" from the DFS topological sort.
A value in the other range would act as the "temporary mark".

We might not need to store the output "topologically-sorted" graph, because
we're using generation numbers instead anyway.

We can assign generations from the lowest to highest number as we unwind the
traversal stack. If we hit a node with a permanent mark (generation number
already assigned), we use the max(target_node_generation - 1,
currently_planned_generation) for the planned generation number.

Now, regarding WCC IDs ("component" numbers).

Aside:
    BTW, can we somehow embed them into the generation number? Probably not,
    as they would have uneven space for their generation numbers, and we'll
    have to move them around constantly.

So, let's say we start with a WCC ID 0. For each new DFS we increment it and
assign the result to the nodes we're traversing which don't have an ID
assigned (are zero). Upon encountering a non-zero ID, add it to a list of IDs
belonging
---

Create an array of node indices able to accommodate all nodes (maybe minus
one?). Use it as a stack to remember the nodes to go back to during the DFS.

Store the number of already traversed outgoing edges in the node's
"generation".

Store the node's generation in "generation", once all edges are traversed and
we are returning to the source node.

Use two distinct ranges of "generation" to store traversal state and the
generation itself. Both excluding zero.

Thus a value in the second range (the actual generation)
would also act as the "permanent mark" from the DFS topological sort.
A value in the other range would act as the "temporary mark".

We might not need to store the output "topologically-sorted" graph, because
we're using generation numbers instead anyway.

We can assign generations from the lowest to highest number as we unwind the
traversal stack. If we hit a node with a permanent mark (generation number
already assigned), we use the max(target_node_generation - 1,
currently_planned_generation) for the planned generation number.

Now, regarding WCC IDs ("component" numbers).

Aside:
    BTW, can we somehow embed them into the generation number? Probably not,
    as they would have uneven space for their generation numbers, and we'll
    have to move them around constantly.

So, let's say we start with a WCC ID 0. For each new DFS we increment it and
assign the result to the nodes we're traversing which don't have an ID
assigned (are zero). Upon encountering a non-zero ID, add it to a list of IDs
belonging

OK, no.

How about we have an array of WCC IDs, one for each node, with the same order
as the nodes? As we do our generation number-assignment DFS, we can assign the
same WCC ID to nodes traversed in a single DFS.

No, wait.

How about we have a linked list with one element per node, with elements
stored in the same order. Then, as we do a single DFS, we keep adding new
nodes to the end of the list. If we hit an already-traversed node,

----

OK, scratch all that. ChatGPT says we can use an (iterative) LPA (Label
Propagation Algorithm) for this. Kinda what I was thinking about before.
However, he's not clear about handling edge directions there. From its
description it won't work.

How about this:
* We start with a WCC ID 0
* For each node in the graph
*   If we haven't traversed this node before
*       Increment the WCC ID
*       Start a DFS supplying it with the WCC ID, which:
*           Assigns this ID to every node without one.
*

Actually, after talking to Panos, I think we could go through finding the DAG
roots by calculating the in-degree of all nodes, storing that in the
"component" members. Then go over every node and do DFS from each that has
zero in-degree, giving it the next component ID to put into all the
"component" members, and assigning the generation number, as well as detecting
cycles. If we find no nodes with zero in-degree, that means there is a cycle
in the graph.

Now, which would be the best state to do it in? Indexed, I guess.
---
TODO: Consider storing node hashes in a separate array, avoiding the hassle of
      using flexible array members.

---

No, there's really no difference between finding root nodes first and not
finding them, for identifying WCCs.

We can always invert the graph and walk the pair of them as an undirected
graph to identify the WCCs.

But let's try something else for a bit longer. We could save on memory for the
normal case, where there are not many WCCs in each created bundle/file.

So, again, say we start with a WCC ID 0
We create an empty array of connected WCC IDs, where each ID's
entry is at the index of its ID-1, and stores the node the WCC was started at,
as well as the ID of the next connected WCC, or zero, if none.
    For each node in the graph
        If the node's component is zero
            Increment WCC ID
            Add an entry to WCC ID list with the next ID set to zero,
            and the current node as the start one.
            Do a DFS assigning the WCC ID to each node with zero component
            (as well as assigning generation numbers)
            If a node with non-zero component is encountered during that:
                If its next WCC ID is not this WCC ID already:
                    Take its next WCC ID and put it into this ID's entry as the
                    next one.
                    Assign this WCC ID into the found entry as the next one.
    Set current WCC ID to zero
    For each WCC ID array element:
        If its next WCC ID is not zero:
            Set current WCC ID to its WCC ID
            For each item in the linked list starting from the next one
                Do a DFS from the starting node, for all nodes with the same
                WCC ID set the current WCC ID
        
The worst case for the memory use is when e.g. every node we traverse points
back to at least one of the previous nodes. For that case we'll need the WCC
ID array length to be the same as the number of nodes. Each item in that array
being 64 bits.

---
Can we avoid allocating the WCC ID linked list, if we simply repeatedly walk
the graph?

Like, take a node, walk down the whole reachable subgraph, find the
maximum/minimum WCC ID among the nodes, then walk it again to assign that
maximum/minimum to everything.

---

OK, let's do it simple for now, just invert the graph.
TODO: Consider renaming `extra_edges` to `target_indexes`.
---
generation == 0 -> no mark
0 < generation <= 0x7fffffff -> generation (permanent mark)
0x7fffffff < generation <= 0xffffffff -> next edge (temporary mark)
component -> the node to return to
---
TODO: Test enumerating with extra edges
TODO: Support "hashless" bundles - ones with hash_len == 0
